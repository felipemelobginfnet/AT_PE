
import streamlit as st
import pandas as pd
import yaml
import json
import plotly.express as px
from PIL import Image
import faiss
from transformers import AutoTokenizer, AutoModel
import os
from dotenv import load_dotenv
import google.generativeai as genai

# Carrega variÃ¡veis de ambiente
load_dotenv()

GEMINI_TOKEN = os.getenv("GEMINI_TOKEN")
# ConfiguraÃ§Ã£o inicial do Streamlit
#st.set_page_config(
#    page_title="Dashboard CÃ¢mara dos Deputados",
#    layout="wide"
#)

# CÃ³digo da aba VisÃ£o Geral (Chain-of-Thought)

import streamlit as st
import yaml
import json
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

st.set_page_config(
    page_title="Dashboard Parlamentar",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Etapa 1 - AnÃ¡lise Inicial
# Bibliotecas
# - streamlit
# - yaml
# - json
# - numpy
# - matplotlib

# Estrutura bÃ¡sica
st.sidebar.title("ConfiguraÃ§Ãµes")
st.sidebar.markdown("---")
aba = st.sidebar.selectbox("Selecione uma aba", ["Overview", "DistribuiÃ§Ã£o de Deputados", "Despesas", "ProposiÃ§Ãµes"])

# Etapa 2 - Desenvolvimento da Aba VisÃ£o Geral

# TÃ­tulo e descriÃ§Ã£o
if aba == "Overview":
    st.title("Overview do Parlamento")
    st.markdown("Este dashboard fornece uma visÃ£o geral do Parlamento Brasileiro, com informaÃ§Ãµes sobre a distribuiÃ§Ã£o de deputados por partido, regiÃ£o e gÃªnero.")

    # Carregando config.yaml
    with open("config.yaml", "r") as f:
        config = yaml.safe_load(f)

    st.markdown("### Resumo")
    st.markdown(config['overview_summary'])
    # GrÃ¡fico de barras
    image = Image.open("docs/distribuicao_deputados.png")
    st.image(image, caption="DistribuiÃ§Ã£o de Deputados por Partido", use_column_width=True)

    # Insights
    with open("data/insights_distribuicao_deputados.json", "r") as f:
        insights = json.load(f)

    st.markdown(f"**Insight 1:** {insights['insight1']}")
    st.markdown(f"**Insight 2:** {insights['insight2']}")

# Etapa 3 - ImplementaÃ§Ã£o Final
# Tratamento de erros
try:
    with open("config.yaml", "r") as f:
        config = yaml.safe_load(f)
    with open("data/insights_distribuicao_deputados.json", "r") as f:
        insights = json.load(f)
except FileNotFoundError as e:
    st.error(f"Arquivo nÃ£o encontrado: {e}")
except Exception as e:
    st.error(f"Erro desconhecido: {e}")

# ComentÃ¡rios e organizaÃ§Ã£o
# Os comentÃ¡rios estÃ£o devidamente dentro da sintaxe do Python
# O cÃ³digo estÃ¡ organizado com indentaÃ§Ã£o e espaÃ§amento adequados


# CÃ³digo das abas Despesas e ProposiÃ§Ãµes (Batch Prompting)

import pandas as pd
import altair as alt
import streamlit as st
import json

# Carregar dados
with open('data/insights_despesas_deputados.json') as f:
    insights_despesas = json.load(f)
df_despesas = pd.read_parquet('data/serie_despesas_diarias_deputados.parquet')
df_proposicoes = pd.read_parquet('data/proposicoes_deputados.parquet')
df_deputados = pd.read_parquet('data/deputados.parquet')

with open('data/sumarizacao_proposicoes.json') as f:
        sumarizacao_proposicoes = json.load(f)
with open('data/insights_distribuicao_deputados.json') as f:
        distribuicao_deputados = json.load(f)        

if aba == "Despesas":
    
    # Aba Despesas
    st.header('Despesas')

    # Exibir insights
    st.subheader('Insights sobre Despesas')
    st.write(insights_despesas)

    # Caixa de seleÃ§Ã£o para escolha do deputado
    deputados = df_despesas['nome'].unique()
    deputado_selecionado = st.selectbox('Deputado:', deputados)

    # GrÃ¡fico de barras temporal
    df_despesas_deputado = df_despesas[df_despesas['nome'] == deputado_selecionado]
    chart = alt.Chart(df_despesas_deputado).mark_bar().encode(
        x='data',
        y='valor',
        color='categoria'
    )
    st.write(chart)

# Aba ProposiÃ§Ãµes
tokenizer = AutoTokenizer.from_pretrained("neuralmind/bert-base-portuguese-cased")
model = AutoModel.from_pretrained("neuralmind/bert-base-portuguese-cased")

# Carregar a base de dados FAISS
index = faiss.read_index("data/proposicoes_index.faiss")
vectors = pd.read_parquet('data/proposicoes_vetorizadas.parquet').values

# FunÃ§Ã£o para realizar busca no FAISS
def busca_faiss(pergunta):
    """Realiza busca semÃ¢ntica no FAISS"""
    try:
        tokens_pergunta = tokenizer(
            pergunta, 
            return_tensors="np", 
            truncation=True, 
            padding=True, 
            max_length=512
        )
        vetor_pergunta = model(**tokens_pergunta).last_hidden_state.mean(axis=1).squeeze(0).numpy()
        distancias, indices = index.search(np.array([vetor_pergunta]), k=5)
        return distancias, indices
    except Exception as erro:
        print(f"Erro na busca FAISS: {erro}")
        return None, None

# FunÃ§Ã£o para analisar resposta
def analisar_resposta(distancias, indices):
    """Analisa resultados da busca FAISS"""
    try:
        if len(distancias) > 0 and distancias[0][0] < 0.5:
            indice_resultado = indices[0][0]
            return vectors[indice_resultado]
        return "Nenhuma informaÃ§Ã£o relevante encontrada."
    except Exception as erro:
        print(f"Erro na anÃ¡lise da resposta: {erro}")
        return None

def configurar_modelo_ia():
    """Configura o modelo Gemini Pro"""
    try:
        genai.configure(api_key=GEMINI_TOKEN)
        configuracoes_seguranca = {
            "HARM_CATEGORY_HARASSMENT": "BLOCK_NONE",
            "HARM_CATEGORY_HATE_SPEECH": "BLOCK_NONE",
            "HARM_CATEGORY_SEXUALLY_EXPLICIT": "BLOCK_NONE",
            "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_NONE"
        }
        
        configuracoes_geracao = {
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 40
        }
        
        modelo = genai.GenerativeModel(
            "gemini-pro",
            safety_settings=configuracoes_seguranca,
            generation_config=configuracoes_geracao
        )
        return modelo
    except Exception as erro:
        print(f"Erro ao configurar modelo: {erro}")
        return None
def gerar_contexto(pergunta):
    """Gera contexto combinando dados relevantes"""
    distancias, indices = busca_faiss(pergunta)
    if indices is not None:
        proposicoes_relevantes = df_proposicoes.iloc[indices[0]]
        
        contexto = f"""
        Dados das ProposiÃ§Ãµes Relevantes:
        {proposicoes_relevantes}
        
        Resumo das ProposiÃ§Ãµes:
        {sumarizacao_proposicoes}
        
        InformaÃ§Ãµes distribuiÃ§Ã£o dos Deputados:
        {distribuicao_deputados}
        
        Despesas dos Deputados:
        {insights_despesas}

        Tabela Deputados:
        {df_deputados}

        Tabela despesas dos Deputados:
        {df_despesas}
        """
        return contexto
    else:
        contexto = f"""

        
        Resumo das ProposiÃ§Ãµes:
        {sumarizacao_proposicoes}
        
        InformaÃ§Ãµes dos Deputados:
        {distribuicao_deputados}
        
        Despesas dos Deputados:
        {insights_despesas}

        Tabela Deputados:
        {df_deputados}

        Tabela despesas dos Deputados:
        {df_despesas}
        """
        return contexto
    
    #return ""
def criar_prompt_self_ask(pergunta, contexto):
    """Cria prompt usando tÃ©cnica Self-Ask"""
    return f"""VocÃª Ã© um assistente especialista em dados da CÃ¢mara dos Deputados.
    
    Para responder Ã  pergunta, siga estas etapas:
    1. Primeiro, identifique qual informaÃ§Ã£o especÃ­fica estÃ¡ sendo solicitada
    2. Depois, verifique quais dados do contexto sÃ£o relevantes
    3. Analise os dados relevantes para encontrar a resposta
    4. Por fim, formule uma resposta clara e objetiva
    
    Pergunta do usuÃ¡rio: {pergunta}
    
    Contexto disponÃ­vel:
    {contexto}
    
    Resposta passo a passo:"""

def interface_proposicoes():
    """Interface principal da aba ProposiÃ§Ãµes"""
    st.header("ProposiÃ§Ãµes")

    # ExibiÃ§Ã£o dos dados bÃ¡sicos
    st.subheader("ProposiÃ§Ãµes por Deputado")
    st.dataframe(df_proposicoes)

    # Filtros
    st.subheader("Filtrar ProposiÃ§Ãµes")
    colunas = df_proposicoes.columns.tolist()
    coluna_selecionada = st.selectbox("Selecione a coluna para filtrar:", colunas)

    if coluna_selecionada:
        valores = df_proposicoes[coluna_selecionada].dropna().unique().tolist()
        valor_selecionado = st.selectbox(f"Selecione o valor para filtrar em {coluna_selecionada}:", valores)
        df_filtrado = df_proposicoes[df_proposicoes[coluna_selecionada] == valor_selecionado]
        st.dataframe(df_filtrado)

    # Assistente Virtual
    st.subheader("Assistente Virtual para AnÃ¡lise de Dados da CÃ¢mara dos Deputados")
    st.markdown("FaÃ§a sua pergunta sobre proposiÃ§Ãµes, despesas e deputados.")

    # Chat
    if "mensagens" not in st.session_state:
        st.session_state.mensagens = []

    pergunta = st.text_input("Digite sua pergunta:", key="campo_pergunta")

    if st.button("Enviar"):
        if pergunta:
            # Adiciona pergunta do usuÃ¡rio ao histÃ³rico
            st.session_state.mensagens.append({"role": "user", "content": pergunta})

            try:
                # Gera resposta
                contexto = gerar_contexto(pergunta)
                prompt = criar_prompt_self_ask(pergunta, contexto)
                modelo = configurar_modelo_ia()
                
                if modelo:
                    resposta = modelo.generate_content(prompt)
                    texto_resposta = resposta.text
                    
                    # Adiciona resposta ao histÃ³rico
                    st.session_state.mensagens.append({"role": "assistant", "content": texto_resposta})
                else:
                    st.error("Erro ao configurar o modelo de IA.")
            except Exception as erro:
                st.error(f"Erro ao processar a pergunta: {erro}")

    # Exibe histÃ³rico do chat
    for msg in st.session_state.mensagens:
        if msg["role"] == "user":
            st.write("VocÃª:", msg["content"])
        else:
            st.write("Assistente:", msg["content"]) 

    st.subheader("Resumo das ProposiÃ§Ãµes")
    st.write(sumarizacao_proposicoes)            

    


if __name__ == "__main__":
    interface_proposicoes()